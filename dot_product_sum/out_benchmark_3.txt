Trittention Kernel Benchmark
==================================================

==================== Configuration 1 ====================
Benchmarking configuration:
  Batch size: 2, Seq len: 64
  Heads: 4, Head dim: 32
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: True

Benchmarking Triton implementation...
  Forward:  0.086 ± 0.004 ms
  Backward: 0.393 ± 0.007 ms
  Total:    0.479 ms

Benchmarking PyTorch implementation...
  Forward:  0.685 ± 0.121 ms
  Backward: 0.658 ± 0.166 ms
  Total:    1.343 ms

Triton speedup: 2.81x

==================== Configuration 2 ====================
Benchmarking configuration:
  Batch size: 2, Seq len: 128
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: True

Benchmarking Triton implementation...
  Forward:  0.105 ± 0.004 ms
  Backward: 0.450 ± 0.025 ms
  Total:    0.555 ms

Benchmarking PyTorch implementation...
  Forward:  7.345 ± 15.672 ms
  Backward: 5.724 ± 16.539 ms
  Total:    13.069 ms

Triton speedup: 23.53x

==================== Configuration 3 ====================
Benchmarking configuration:
  Batch size: 4, Seq len: 256
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: True

Benchmarking Triton implementation...
  Forward:  0.263 ± 0.023 ms
  Backward: 1.162 ± 0.070 ms
  Total:    1.425 ms

Benchmarking PyTorch implementation...
  Forward:  36.051 ± 11.302 ms
  Backward: 24.494 ± 0.215 ms
  Total:    60.545 ms

Triton speedup: 42.50x

==================== Configuration 4 ====================
Benchmarking configuration:
  Batch size: 4, Seq len: 256
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.bfloat16
  Benchmark PyTorch: True

Benchmarking Triton implementation...
  Forward:  0.158 ± 0.004 ms
  Backward: 0.582 ± 0.031 ms
  Total:    0.740 ms

Benchmarking PyTorch implementation...
  Forward:  33.086 ± 16.352 ms
  Backward: 14.487 ± 0.050 ms
  Total:    47.573 ms

Triton speedup: 64.26x

==================== Configuration 5 ====================
Benchmarking configuration:
  Batch size: 4, Seq len: 256
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: True

Benchmarking Triton implementation...
  Forward:  0.208 ± 0.005 ms
  Backward: 0.800 ± 0.029 ms
  Total:    1.008 ms

Benchmarking PyTorch implementation...
  Forward:  39.639 ± 13.603 ms
  Backward: 24.447 ± 0.115 ms
  Total:    64.086 ms

Triton speedup: 63.59x

==================== Configuration 6 ====================
Benchmarking configuration:
  Batch size: 4, Seq len: 256
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.bfloat16
  Benchmark PyTorch: True

Benchmarking Triton implementation...
  Forward:  0.137 ± 0.004 ms
  Backward: 0.530 ± 0.034 ms
  Total:    0.667 ms

Benchmarking PyTorch implementation...
  Forward:  32.499 ± 16.248 ms
  Backward: 17.724 ± 7.904 ms
  Total:    50.223 ms

Triton speedup: 75.32x

==================== Configuration 7 ====================
Benchmarking configuration:
  Batch size: 2, Seq len: 512
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: True

Benchmarking Triton implementation...
  Forward:  0.667 ± 0.023 ms
  Backward: 2.211 ± 0.022 ms
  Total:    2.878 ms

Benchmarking PyTorch implementation...
  Error: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.19 GiB of which 5.18 GiB is free. Process 180480 has 74.00 GiB memory in use. Of the allocated memory 66.38 GiB is allocated by PyTorch, and 6.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  Error traceback: Traceback (most recent call last):
  File "/root/tritt/benchmark_trittention.py", line 246, in run_benchmark
    fwd_time_mean, fwd_time_std = benchmark_forward_pass(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/tritt/benchmark_trittention.py", line 50, in benchmark_forward_pass
    output = model(q, k1, k2, v1, v2)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/tritt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/tritt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/tritt/trittention_pytorch.py", line 73, in forward
    attn_score2 = torch.exp(attn_score) / L.unsqueeze(-1)
                  ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.19 GiB of which 5.18 GiB is free. Process 180480 has 74.00 GiB memory in use. Of the allocated memory 66.38 GiB is allocated by PyTorch, and 6.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


==================== Configuration 8 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 1024
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  2.115 ± 0.012 ms
  Backward: 6.567 ± 0.056 ms
  Total:    8.682 ms

==================== Configuration 9 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 2048
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  9.573 ± 0.130 ms
  Backward: 27.409 ± 0.092 ms
  Total:    36.983 ms

==================== Configuration 10 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 2048
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  2.712 ± 0.017 ms
  Backward: 8.448 ± 0.052 ms
  Total:    11.160 ms

==================== Configuration 11 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 4096
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  60.458 ± 0.094 ms
  Backward: 174.081 ± 0.224 ms
  Total:    234.539 ms

==================== Configuration 12 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 4096
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  9.159 ± 0.122 ms
  Backward: 28.522 ± 0.074 ms
  Total:    37.681 ms

==================== Configuration 13 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 8192
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  282.460 ± 0.201 ms
  Backward: 831.150 ± 0.613 ms
  Total:    1113.610 ms

==================== Configuration 14 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 8192
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.float32
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  33.301 ± 0.035 ms
  Backward: 106.335 ± 0.118 ms
  Total:    139.636 ms

==================== Configuration 15 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 4096
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.bfloat16
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  26.633 ± 0.179 ms
  Backward: 63.503 ± 0.160 ms
  Total:    90.136 ms

==================== Configuration 16 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 4096
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.bfloat16
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  4.680 ± 0.042 ms
  Backward: 10.999 ± 0.097 ms
  Total:    15.678 ms

==================== Configuration 17 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 8192
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.bfloat16
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  123.573 ± 0.106 ms
  Backward: 301.993 ± 0.117 ms
  Total:    425.566 ms

==================== Configuration 18 ====================
Large seq_len detected, disabling PyTorch benchmarking to avoid OOM
Benchmarking configuration:
  Batch size: 1, Seq len: 8192
  Heads: 8, Head dim: 64
  Causal: True, Device: cuda, Dtype: torch.bfloat16
  Benchmark PyTorch: False

Benchmarking Triton implementation...
  Forward:  16.861 ± 0.176 ms
  Backward: 39.614 ± 0.034 ms
  Total:    56.474 ms

==================================================
BENCHMARK SUMMARY
==================================================
Config   Seq Len  K_diff   Dtype        Triton (ms)  PyTorch (ms)  Speedup 
--------------------------------------------------------------------------------
1        64       1024     float32      0.5          1.3           2.81x   
2        128      1024     float32      0.6          13.1          23.53x  
3        256      1024     float32      1.4          60.5          42.50x  
4        256      1024     bfloat16     0.7          47.6          64.26x  
5        256      40       float32      1.0          64.1          63.59x  
6        256      40       bfloat16     0.7          50.2          75.32x  
7        512      N/A      float32      2.9          SKIP/FAIL     N/A     
8        1024     N/A      float32      8.7          SKIP/FAIL     N/A     
9        2048     1024     float32      37.0         SKIP/FAIL     N/A     
10       2048     128      float32      11.2         SKIP/FAIL     N/A     
11       4096     2048     float32      234.5        SKIP/FAIL     N/A     
12       4096     128      float32      37.7         SKIP/FAIL     N/A     
13       8192     2048     float32      1113.6       SKIP/FAIL     N/A     
14       8192     128      float32      139.6        SKIP/FAIL     N/A     
15       4096     2048     bfloat16     90.1         SKIP/FAIL     N/A     
16       4096     128      bfloat16     15.7         SKIP/FAIL     N/A     
17       8192     2048     bfloat16     425.6        SKIP/FAIL     N/A     
18       8192     128      bfloat16     56.5         SKIP/FAIL     N/A     

==================================================
Benchmark completed!
